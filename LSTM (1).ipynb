{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahSvfNppl5gE",
        "colab_type": "text"
      },
      "source": [
        "# Long Short-Term Memory (LSTM) Model for Daily Precipitation Prediction \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Last Updated : 27th February, 2020"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Io-7TtrmabF",
        "colab_type": "text"
      },
      "source": [
        "Importing required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6Ts1nIsmlGp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dropout\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLNrcLpgmnPw",
        "colab_type": "text"
      },
      "source": [
        "Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUOVga23m1oA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Prepare the data\n",
        "df = pd.read_csv(r\"gdrive/My Drive/tp_1980_1990.csv\")\n",
        "outfl = open(r\"gdrive/My Drive/LSTM_result.csv\", 'w')\n",
        "\n",
        "outfl.write(\"CombinationID, Timestep, Dropout, Batch Size, Layers, Units, Accuracy\\n\")\n",
        "outfl.close()\n",
        "\n",
        "ppts = df['total_precipitation'].values\n",
        "\n",
        "def preparedata(x, ts=60):\n",
        "    x_train = []\n",
        "    y_train = []\n",
        "    \n",
        "\n",
        "    x = np.array(x)\n",
        "\n",
        "    for v in tqdm(range(ts, len(x))):\n",
        "        xt = x[v-ts:v]\n",
        "        yt = x[v]\n",
        "\n",
        "        x_train.append(xt)\n",
        "        y_train.append(yt)\n",
        "    \n",
        "    return np.array(x_train), np.array(y_train)\n",
        "\n",
        "\n",
        "\n",
        "vals = np.array(ppts)\n",
        "rfval = vals.reshape(-1, 1)\n",
        "\n",
        "#Data normalisation\n",
        "sc = MinMaxScaler(feature_range = (0, 1))\n",
        "training_set_scaled = sc.fit_transform(rfval)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FplN7TL0DVl7",
        "colab_type": "code",
        "outputId": "ba7f86b8-624b-4fa0-b509-e8f8aa8f5c48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 685
        }
      },
      "source": [
        "\n",
        "#Model\n",
        "timesteps = [90,180,365,730,1095,1460,1825]\n",
        "dropouts = [0.1,0.2,0.5,0.7,0.9]\n",
        "batchsize = [100,200,500,1000]\n",
        "layers = [1,2,3,4,5,6,7,8,9,10]\n",
        "units = [25,50,100,150,200]\n",
        "\n",
        "instance = 0\n",
        "for ts in timesteps:\n",
        "    for do in dropouts:\n",
        "        for batch_size in batchsize:\n",
        "            for n_layer in layers:\n",
        "                for unit in units:\n",
        "\n",
        "                    print(\"Current Configuration\\n---------------------------------------------\")\n",
        "                    print(\"Combination : \" + str(instance) + \" out of 7000\")\n",
        "                    print(\"Timestep : \" + str(ts))\n",
        "                    print(\"Droup out rate : \" + str(do))\n",
        "                    print(\"Batch size : \" + str(batch_size))\n",
        "                    print(\"Number of deep layers : \"  + str(n_layer))\n",
        "                    print(\"Neurons in each layer : \" + str(unit))\n",
        "                    print(\"---------------------------------------------\")\n",
        "\n",
        "\n",
        "                    #Prepare datset according to timestep\n",
        "                    x,y = preparedata(training_set_scaled, ts=ts)\n",
        "                    x_train = x[:10000]\n",
        "                    y_train = y[:10000]\n",
        "\n",
        "\n",
        "                    #Prepare Validation Set\n",
        "                    x_test,y_test = preparedata(ppts[10000:], ts=ts)\n",
        "\n",
        "\n",
        "                    x_train = x_train.reshape((len(x_train), ts, 1))\n",
        "                    x_test = x_test.reshape((len(x_test), ts, 1))\n",
        "\n",
        "                    #Prepare Model\n",
        "\n",
        "\n",
        "                    # Initialising the RNN\n",
        "                    regressor = Sequential()\n",
        "\n",
        "                    # Adding the first LSTM layer and some Dropout regularisation\n",
        "                    regressor.add(LSTM(units = unit, return_sequences = True, input_shape = (x_train.shape[1], 1)))\n",
        "                    regressor.add(Dropout(do))\n",
        "\n",
        "                    for _ in range(n_layer):\n",
        "\n",
        "                        # Adding a  LSTM layer and some Dropout regularisation\n",
        "                        regressor.add(LSTM(units = unit, return_sequences = True))\n",
        "                        regressor.add(Dropout(do))\n",
        "\n",
        "                        \n",
        "\n",
        "\n",
        "                    regressor.add(LSTM(units = unit))\n",
        "                    regressor.add(Dropout(do))\n",
        "\n",
        "                    # Adding the output layer\n",
        "                    regressor.add(Dense(units = 1))\n",
        "\n",
        "                    # Compiling the RNN\n",
        "                    regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
        "\n",
        "                    regressor.fit(x_train, y_train, epochs = 5, batch_size = batch_size, validation_split=0.2)\n",
        "\n",
        "                    y_pred = regressor.predict(x_test, batch_size = batch_size)\n",
        "\n",
        "                    import matplotlib.pyplot as plt\n",
        "\n",
        "                    plt.scatter(y_test, y_pred)\n",
        "                    plt.plot(y_test, label=\"Test\")\n",
        "                    plt.plot(y_pred, label=\"Prediction\")\n",
        "                    plt.legend()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                    #Write output in text file...\n",
        "                    #outfl = open(r\"gdrive/My Drive/LSTM_result.csv\", 'a')\n",
        "\n",
        "                    #outfl.write(\"{0}, {1}, {2}, {3}, {4}, {5}, {6}\\n\".format(instance,ts,do,batch_size,n_layer,unit,acc))\n",
        "                    #outfl.close()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 14308/14308 [00:00<00:00, 794294.17it/s]\n",
            "100%|██████████| 4308/4308 [00:00<00:00, 845059.47it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Current Configuration\n",
            "---------------------------------------------\n",
            "Combination : 0 out of 7000\n",
            "Timestep : 180\n",
            "Droup out rate : 0.2\n",
            "Batch size : 100\n",
            "Number of deep layers : 10\n",
            "Neurons in each layer : 100\n",
            "---------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 8000 samples, validate on 2000 samples\n",
            "Epoch 1/5\n",
            "8000/8000 [==============================] - 289s 36ms/step - loss: 0.0027 - val_loss: 0.0024\n",
            "Epoch 2/5\n",
            "1800/8000 [=====>........................] - ETA: 3:19 - loss: 0.0027"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-555c69518327>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0mregressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'mean_squared_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m \u001b[0mregressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}